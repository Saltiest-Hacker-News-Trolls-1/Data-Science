{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "data = open('toy_data', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scene_Cast2</td>\n",
       "      <td>How do you all monitor air quality?&lt;p&gt;I person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leonroy</td>\n",
       "      <td>Any idea how good the Awair monitors are? I re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spladug</td>\n",
       "      <td>The South Coast (California) Air Quality Manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PowerfulWizard</td>\n",
       "      <td>Anyone want to review this table and suggest a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bearforcenine</td>\n",
       "      <td>Based on my reading of the table, the PurpleAi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user                                            comment\n",
       "0     Scene_Cast2  How do you all monitor air quality?<p>I person...\n",
       "1         leonroy  Any idea how good the Awair monitors are? I re...\n",
       "2         spladug  The South Coast (California) Air Quality Manag...\n",
       "3  PowerfulWizard  Anyone want to review this table and suggest a...\n",
       "4   bearforcenine  Based on my reading of the table, the PurpleAi..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ast.literal_eval(data))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.tokenizers.word_tokenizer.WordTokenizer at 0x7f6a7ab8cb38>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
    "WordTokenizer('okay this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    How do you all monitor air quality?I personall...\n",
       "3    Anyone want to review this table and suggest a...\n",
       "4    Based on my reading of the table, the PurpleAi...\n",
       "6    >*Field R2Under the table is definition of R^2...\n",
       "7               Isn't the table sorted alphabetically?\n",
       "Name: cleanComment, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def removeLinks(data):\n",
    "    if 'href' in data: # re.sub('https\\S+', '', data)\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "df['cleanComment']=df['comment'].apply(removeLinks).dropna().apply(lambda x: BeautifulSoup(x).get_text())\n",
    "df = df.dropna()\n",
    "df['cleanComment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "ratings_df = pd.DataFrame(list(map(analyzer.polarity_scores, df.cleanComment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>comment</th>\n",
       "      <th>cleanComment</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scene_Cast2</td>\n",
       "      <td>How do you all monitor air quality?&lt;p&gt;I person...</td>\n",
       "      <td>How do you all monitor air quality?I personall...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PowerfulWizard</td>\n",
       "      <td>Anyone want to review this table and suggest a...</td>\n",
       "      <td>Anyone want to review this table and suggest a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.8425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bearforcenine</td>\n",
       "      <td>Based on my reading of the table, the PurpleAi...</td>\n",
       "      <td>Based on my reading of the table, the PurpleAi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9dl</td>\n",
       "      <td>&amp;gt;*Field R2&lt;p&gt;Under the table is definition ...</td>\n",
       "      <td>&gt;*Field R2Under the table is definition of R^2...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bearforcenine</td>\n",
       "      <td>Isn&amp;#x27;t the table sorted alphabetically?</td>\n",
       "      <td>Isn't the table sorted alphabetically?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user                                            comment  \\\n",
       "0     Scene_Cast2  How do you all monitor air quality?<p>I person...   \n",
       "1  PowerfulWizard  Anyone want to review this table and suggest a...   \n",
       "2   bearforcenine  Based on my reading of the table, the PurpleAi...   \n",
       "3             9dl  &gt;*Field R2<p>Under the table is definition ...   \n",
       "4   bearforcenine        Isn&#x27;t the table sorted alphabetically?   \n",
       "\n",
       "                                        cleanComment  neg    neu    pos  \\\n",
       "0  How do you all monitor air quality?I personall...  0.0  1.000  0.000   \n",
       "1  Anyone want to review this table and suggest a...  0.0  0.799  0.201   \n",
       "2  Based on my reading of the table, the PurpleAi...  0.0  1.000  0.000   \n",
       "3  >*Field R2Under the table is definition of R^2...  0.0  0.922  0.078   \n",
       "4             Isn't the table sorted alphabetically?  0.0  0.814  0.186   \n",
       "\n",
       "   compound  \n",
       "0    0.0000  \n",
       "1    0.8425  \n",
       "2    0.0000  \n",
       "3    0.4939  \n",
       "4    0.6249  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([df, ratings_df], axis=1, join='inner').reset_index(drop=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09423076923076923"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.neg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6a1caa3160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x=df2['neg'], data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vdir(obj):\n",
    "    return [x for x in dir(obj) if not x.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_df = pd.read_csv('data/text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin on your call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone who has Houston tickets, but no one will.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                                                                        content  \n",
       "0  @tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[  \n",
       "1                                  Layin n bed with a headache  ughhhh...waitin on your call...  \n",
       "2                                                           Funeral ceremony...gloomy friday...  \n",
       "3                                                          wants to hang out with friends SOON!  \n",
       "4        @dannycastillo We want to trade with someone who has Houston tickets, but no one will.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion datasets\n",
    "# ISEAR\n",
    "# Tales\n",
    "# Reddit\n",
    "# Movie Reviews\n",
    "# ANET\n",
    "# SemEval\n",
    "# DepecheMood # largest\n",
    "# Word2Vec\n",
    "# Glove # \n",
    "# Elmo for contextualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    -0.261480\n",
       "2     0.264400\n",
       "3     0.448760\n",
       "4     0.159900\n",
       "5    -0.476920\n",
       "6    -0.319420\n",
       "7     2.156100\n",
       "8    -0.526340\n",
       "9    -0.568540\n",
       "10    0.288940\n",
       "11   -0.309100\n",
       "12    0.694520\n",
       "13   -5.600800\n",
       "14   -0.184110\n",
       "15    0.075871\n",
       "16    0.696570\n",
       "17    0.359980\n",
       "18   -0.647240\n",
       "19    0.347930\n",
       "20   -0.593300\n",
       "21    0.576650\n",
       "22    0.305560\n",
       "23    0.488150\n",
       "24   -0.110320\n",
       "25   -0.780190\n",
       "Name: when, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GloVe vector representations for words\n",
    "glove_size =  25\n",
    "glove_file = f'data/glove.twitter.27B.{glove_size}d.txt'\n",
    "embeddings = pd.read_table(glove_file, sep=\" \", index_col=0, header=None,quoting=3) # 3 is quote none\n",
    "\n",
    "embeddings.loc['when']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193514, 25)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt;</th>\n",
       "      <td>0.62415</td>\n",
       "      <td>0.624760</td>\n",
       "      <td>-0.082335</td>\n",
       "      <td>0.201010</td>\n",
       "      <td>-0.137410</td>\n",
       "      <td>-0.11431</td>\n",
       "      <td>0.77909</td>\n",
       "      <td>2.6356</td>\n",
       "      <td>-0.463510</td>\n",
       "      <td>0.57465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.94879</td>\n",
       "      <td>-0.017336</td>\n",
       "      <td>-0.863490</td>\n",
       "      <td>-1.33480</td>\n",
       "      <td>0.046811</td>\n",
       "      <td>0.36999</td>\n",
       "      <td>-0.57663</td>\n",
       "      <td>-0.48469</td>\n",
       "      <td>0.400780</td>\n",
       "      <td>0.75345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.69586</td>\n",
       "      <td>-1.146900</td>\n",
       "      <td>-0.417970</td>\n",
       "      <td>-0.022311</td>\n",
       "      <td>-0.023801</td>\n",
       "      <td>0.82358</td>\n",
       "      <td>1.22280</td>\n",
       "      <td>1.7410</td>\n",
       "      <td>-0.909790</td>\n",
       "      <td>1.37250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.57058</td>\n",
       "      <td>-0.508610</td>\n",
       "      <td>-0.165750</td>\n",
       "      <td>-0.98153</td>\n",
       "      <td>-0.821300</td>\n",
       "      <td>0.24333</td>\n",
       "      <td>-0.14482</td>\n",
       "      <td>-0.67877</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>0.40833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>1.12420</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>-0.037362</td>\n",
       "      <td>0.100460</td>\n",
       "      <td>0.119230</td>\n",
       "      <td>-0.30009</td>\n",
       "      <td>1.09380</td>\n",
       "      <td>2.5370</td>\n",
       "      <td>-0.072802</td>\n",
       "      <td>1.04910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.99347</td>\n",
       "      <td>-0.650720</td>\n",
       "      <td>-0.030948</td>\n",
       "      <td>-1.08170</td>\n",
       "      <td>-0.647010</td>\n",
       "      <td>0.32341</td>\n",
       "      <td>-0.41612</td>\n",
       "      <td>-0.52680</td>\n",
       "      <td>-0.047166</td>\n",
       "      <td>0.71549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>0.74056</td>\n",
       "      <td>0.915500</td>\n",
       "      <td>-0.163520</td>\n",
       "      <td>0.358430</td>\n",
       "      <td>0.052660</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>1.04210</td>\n",
       "      <td>2.8073</td>\n",
       "      <td>0.128650</td>\n",
       "      <td>1.04920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.91433</td>\n",
       "      <td>-0.404560</td>\n",
       "      <td>-1.098800</td>\n",
       "      <td>-1.03330</td>\n",
       "      <td>-0.178750</td>\n",
       "      <td>0.37979</td>\n",
       "      <td>-0.25922</td>\n",
       "      <td>-0.74854</td>\n",
       "      <td>0.360010</td>\n",
       "      <td>0.61206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.84705</td>\n",
       "      <td>-1.034900</td>\n",
       "      <td>-0.050419</td>\n",
       "      <td>0.271640</td>\n",
       "      <td>-0.586590</td>\n",
       "      <td>0.99514</td>\n",
       "      <td>0.25267</td>\n",
       "      <td>1.6963</td>\n",
       "      <td>0.103130</td>\n",
       "      <td>0.80073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67015</td>\n",
       "      <td>-0.648120</td>\n",
       "      <td>0.010373</td>\n",
       "      <td>-0.71889</td>\n",
       "      <td>-0.749970</td>\n",
       "      <td>0.24862</td>\n",
       "      <td>0.10319</td>\n",
       "      <td>-1.17320</td>\n",
       "      <td>0.581960</td>\n",
       "      <td>0.33846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5        6        7   \\\n",
       "0                                                                           \n",
       "<user>  0.62415  0.624760 -0.082335  0.201010 -0.137410 -0.11431  0.77909   \n",
       ".       0.69586 -1.146900 -0.417970 -0.022311 -0.023801  0.82358  1.22280   \n",
       ":       1.12420  0.054519 -0.037362  0.100460  0.119230 -0.30009  1.09380   \n",
       "rt      0.74056  0.915500 -0.163520  0.358430  0.052660  0.14560  1.04210   \n",
       ",       0.84705 -1.034900 -0.050419  0.271640 -0.586590  0.99514  0.25267   \n",
       "\n",
       "            8         9        10  ...       16        17        18       19  \\\n",
       "0                                  ...                                         \n",
       "<user>  2.6356 -0.463510  0.57465  ... -0.94879 -0.017336 -0.863490 -1.33480   \n",
       ".       1.7410 -0.909790  1.37250  ... -0.57058 -0.508610 -0.165750 -0.98153   \n",
       ":       2.5370 -0.072802  1.04910  ... -0.99347 -0.650720 -0.030948 -1.08170   \n",
       "rt      2.8073  0.128650  1.04920  ... -0.91433 -0.404560 -1.098800 -1.03330   \n",
       ",       1.6963  0.103130  0.80073  ... -0.67015 -0.648120  0.010373 -0.71889   \n",
       "\n",
       "              20       21       22       23        24       25  \n",
       "0                                                               \n",
       "<user>  0.046811  0.36999 -0.57663 -0.48469  0.400780  0.75345  \n",
       ".      -0.821300  0.24333 -0.14482 -0.67877  0.706100  0.40833  \n",
       ":      -0.647010  0.32341 -0.41612 -0.52680 -0.047166  0.71549  \n",
       "rt     -0.178750  0.37979 -0.25922 -0.74854  0.360010  0.61206  \n",
       ",      -0.749970  0.24862  0.10319 -1.17320  0.581960  0.33846  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['<user>', '.', ':', 'rt', ',', '<repeat>', '<hashtag>', '<number>',\n",
       "       '<url>', '!', 'i', 'a', '\"', 'the', '?', 'you', 'to', '(', '<allcaps>',\n",
       "       '<elong>'],\n",
       "      dtype='object', name=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.index[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=25\n",
    "MAXLEN=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text\n",
    "from gensim import corpora\n",
    "import importlib\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "emo_df['tokens'] = emo_df['content'].apply(text.tokenize)\n",
    "\n",
    "def get_token_counts(tokens_col):\n",
    "    token_counts = {}\n",
    "    for tokens in tokens_col:\n",
    "        for token in tokens:\n",
    "            if not token in token_counts.keys():\n",
    "                token_counts[token] = 1\n",
    "            else:\n",
    "                token_counts[token] += 1\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import * # import neural network functions\n",
    "emo_df = load_file('emo_df.pickle')\n",
    "top_words = load_file('top_words.pickle')\n",
    "token_embeddings = load_file('embeddings.pickle')\n",
    "id2word=corpora.Dictionary(emo_df['tokens2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>sinus</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>stupid</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word_index    word  word_count\n",
       "15          15   sinus          10\n",
       "16          16  stupid         492"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words[15:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[</td>\n",
       "      <td>[@tiffanylue, know, listenin, bad, habit, earlier, started, freakin, =, []</td>\n",
       "      <td>[know, listenin, bad, habit, earlier, started, freakin, =, []</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin on your call...</td>\n",
       "      <td>[Layin, n, bed, headache, ughhhh...waitin, call...]</td>\n",
       "      <td>[n, bed, headache]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id sentiment      author  \\\n",
       "0  1956967341     empty  xoshayzers   \n",
       "1  1956967666   sadness   wannamama   \n",
       "\n",
       "                                                                                        content  \\\n",
       "0  @tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[   \n",
       "1                                  Layin n bed with a headache  ughhhh...waitin on your call...   \n",
       "\n",
       "                                                                       tokens  \\\n",
       "0  [@tiffanylue, know, listenin, bad, habit, earlier, started, freakin, =, []   \n",
       "1                         [Layin, n, bed, headache, ughhhh...waitin, call...]   \n",
       "\n",
       "                                                         tokens2  \n",
       "0  [know, listenin, bad, habit, earlier, started, freakin, =, []  \n",
       "1                                             [n, bed, headache]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks good\n",
    "assert((token_embeddings[197] == embeddings.loc['hey'].values).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import utils\n",
    "# Old Model with accuracy of around ~55%\n",
    "# X = token_matrix[0:15_000]\n",
    "# le = LabelEncoder()\n",
    "# y = utils.to_categorical(le.fit_transform(emo_df['sentiment']))\n",
    "\n",
    "# model = create_model(learning_rate=0.005, filter_size=6, out_size=emo_df['sentiment'].nunique())\n",
    "# model.fit(X,y[0:15_000], verbose=True, epochs=10, batch_size=128, validation_split=0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt \n",
    "# n = len(model.history.history['loss'])\n",
    "# plt.plot(range(n), model.history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('emotion.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_test = doc2token_seq('I hate everything aaa I hate you')\n",
    "# pred = model.predict(np.array([my_test]))\n",
    "# le.inverse_transform([np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_test = doc2token_seq('Wish me luck!')\n",
    "# pred = model.predict(np.array([my_test]))\n",
    "# le.inverse_transform([np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Dialog Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dialogues_act_train.txt',\n",
       " 'dialogues_emotion_train.txt',\n",
       " 'dialogues_train.txt']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at Daily Dialog dataset nad see if it can improve the model's accuracy\n",
    "import os\n",
    "dd_dir = 'ijcnlp_dailydialog/train'\n",
    "os.listdir(dd_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_file, emot_file, train_file = map(lambda x: os.path.join(dd_dir, x), os.listdir(dd_dir))\n",
    "act_file = open(act_file, 'r').read().split('\\n')\n",
    "emot_file = open(emot_file, 'r').read().split('\\n')\n",
    "train_file = open(train_file, 'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seqs = []\n",
    "all_emos = []\n",
    "# all_acts = []\n",
    "for line_count, (line_dial, line_emo, line_act) in enumerate(zip(train_file, emot_file, act_file)):\n",
    "    seqs = line_dial.split('__eou__')\n",
    "    seqs = seqs[:-1]\n",
    "\n",
    "    emos = line_emo.split(' ')\n",
    "    emos = emos[:-1]\n",
    "\n",
    "#    acts = line_act.split(' ')\n",
    "#    acts = acts[:-1]\n",
    "    \n",
    "    all_seqs.append(seqs)\n",
    "    all_emos.append(emos)\n",
    "#    all_acts.append(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily dialog dataset\n",
    "import itertools\n",
    "sentences = list(itertools.chain(*all_seqs))\n",
    "emotes = list(itertools.chain(*all_emos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saved to file: id2word.pickle\n",
      "[i] Saved to file: top_words.pickle\n",
      "[i] Saved to file: dd_train.pickle\n",
      "[i] Saved to file: token_seqs.pickle\n",
      "[i] Saved to file: token_embeddings.pickle\n"
     ]
    }
   ],
   "source": [
    "# Loading in the datastes and creating the necssary embeddings\n",
    "SAMPLE_SIZE = 30_000\n",
    "needs_update = False\n",
    "\n",
    "# a bit slow\n",
    "if needs_update:\n",
    "    # Load the emotional dataset\n",
    "    emo_df = load_file('emo_df.pickle')\n",
    "    emo_df = emo_df.replace({'sentiment' : {'empty' : 0, 'sadness' : 5, 'worry' : 3, \n",
    "                                            'fun' : 4, 'happiness' : 4, 'relief' : 4,\n",
    "                                            'love' : 4, 'boredom' : 0, 'anger' : 1, \n",
    "                                            'hate' : 2, 'neutral' : 0, 'enthusiasm' : 4,\n",
    "                                            'surprise' : 6}})\n",
    "    emo_df = emo_df[['sentiment', 'content']].sample(SAMPLE_SIZE)\n",
    "    emo_df = emo_df.rename(columns={'content' : 'sentence', 'sentiment' : 'emote'})\n",
    "    # Load in some other emotional data, focused around the individual\n",
    "    more_emotes = pd.read_csv('data/emotion.data').iloc[:,1:] # skip the id column\n",
    "    more_emotes = more_emotes.replace({'emotions' : \n",
    "                         {'sadness' : 5, \n",
    "                          'joy' : 4, \n",
    "                          'love' : 4,\n",
    "                          'anger' : 1,\n",
    "                          'fear' : 3,\n",
    "                          'surprise' : 6}})\n",
    "    more_emotes = more_emotes.rename(columns={'text' :'sentence', 'emotions' : 'emote'}).sample(SAMPLE_SIZE)\n",
    "    \n",
    "    # Setup the daily dialog dataset, add in the extra data\n",
    "    dd_train = pd.DataFrame({'sentence':sentences,\n",
    "                             'emote':emotes}).sample(SAMPLE_SIZE)\n",
    "    # merge together the datasets\n",
    "    dd_train = dd_train.append(more_emotes)\n",
    "    dd_train = dd_train.append(emo_df)\n",
    "    dd_train['emote'] = dd_train['emote'].astype(int)\n",
    "    dd_train['tokens'] = dd_train['sentence'].apply(text.tokenize)\n",
    "\n",
    "    # filter the tokens only which have embeddings\n",
    "    dd_train['tokens'] = dd_train['tokens'].apply(lambda x: filter_tokens(embeddings.index, x)) \n",
    "    \n",
    "    # Get token embeddings for the words in the corpus and store those for use in the \n",
    "    # embedding layer in the neural net. GloVe (Global Vectors for Word Representation) embeddings.\n",
    "    id2word=corpora.Dictionary(dd_train['tokens'])\n",
    "    id2word.filter_extremes(no_below=3)\n",
    "    update_id2word(id2word)\n",
    "    token_counts = pd.DataFrame(get_token_counts(dd_train['tokens']).items(), columns=['word', 'word_count'])\n",
    "    top_words = pd.DataFrame(id2word.items(), columns=['word_index', 'word'])\n",
    "    top_words = top_words.merge(token_counts, on='word')\n",
    "    # filter out some of the uncommon tokens\n",
    "    dd_train['tokens'] = dd_train['tokens'].apply(lambda x: filter_tokens(top_words['word'].values, x)) \n",
    "    \n",
    "    # get embeddings\n",
    "    token_seqs = dd_train['tokens'].apply(lambda tokens: get_token_seqs(tokens))\n",
    "    token_embeddings = top_words['word'].apply(lambda word: embeddings.loc[word])\n",
    "\n",
    "    # save everything\n",
    "    # update id2word used by get_token_seqs\n",
    "    save_file('id2word.pickle', id2word)\n",
    "    save_file('top_words.pickle', top_words)\n",
    "    save_file('dd_train.pickle', dd_train)\n",
    "    save_file('token_seqs.pickle', token_seqs)\n",
    "    save_file('token_embeddings.pickle', token_embeddings)\n",
    "else:\n",
    "    dd_train = load_file('dd_train.pickle')\n",
    "    top_words = load_file('top_words.pickle')\n",
    "    token_seqs = load_file('token_seqs.pickle')\n",
    "    token_embeddings = load_file('token_embeddings.pickle')\n",
    "    id2word = load_file('id2word.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.84705</td>\n",
       "      <td>-1.034900</td>\n",
       "      <td>-0.050419</td>\n",
       "      <td>0.271640</td>\n",
       "      <td>-0.586590</td>\n",
       "      <td>0.99514</td>\n",
       "      <td>0.25267</td>\n",
       "      <td>1.696300</td>\n",
       "      <td>0.103130</td>\n",
       "      <td>0.80073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.670150</td>\n",
       "      <td>-0.648120</td>\n",
       "      <td>0.010373</td>\n",
       "      <td>-0.71889</td>\n",
       "      <td>-0.749970</td>\n",
       "      <td>0.248620</td>\n",
       "      <td>0.103190</td>\n",
       "      <td>-1.173200</td>\n",
       "      <td>0.58196</td>\n",
       "      <td>0.33846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.69586</td>\n",
       "      <td>-1.146900</td>\n",
       "      <td>-0.417970</td>\n",
       "      <td>-0.022311</td>\n",
       "      <td>-0.023801</td>\n",
       "      <td>0.82358</td>\n",
       "      <td>1.22280</td>\n",
       "      <td>1.741000</td>\n",
       "      <td>-0.909790</td>\n",
       "      <td>1.37250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.570580</td>\n",
       "      <td>-0.508610</td>\n",
       "      <td>-0.165750</td>\n",
       "      <td>-0.98153</td>\n",
       "      <td>-0.821300</td>\n",
       "      <td>0.243330</td>\n",
       "      <td>-0.144820</td>\n",
       "      <td>-0.678770</td>\n",
       "      <td>0.70610</td>\n",
       "      <td>0.40833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.98126</td>\n",
       "      <td>0.458230</td>\n",
       "      <td>0.201170</td>\n",
       "      <td>-0.405540</td>\n",
       "      <td>-0.976080</td>\n",
       "      <td>-0.54941</td>\n",
       "      <td>0.94278</td>\n",
       "      <td>0.072537</td>\n",
       "      <td>-0.371080</td>\n",
       "      <td>0.12274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.723650</td>\n",
       "      <td>-0.501790</td>\n",
       "      <td>-0.495010</td>\n",
       "      <td>-1.10370</td>\n",
       "      <td>-0.997580</td>\n",
       "      <td>0.277280</td>\n",
       "      <td>0.218630</td>\n",
       "      <td>1.034900</td>\n",
       "      <td>0.16152</td>\n",
       "      <td>0.37009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.32929</td>\n",
       "      <td>-0.160370</td>\n",
       "      <td>0.107850</td>\n",
       "      <td>-0.396100</td>\n",
       "      <td>-0.488270</td>\n",
       "      <td>-0.17528</td>\n",
       "      <td>0.23056</td>\n",
       "      <td>-0.491150</td>\n",
       "      <td>-0.065798</td>\n",
       "      <td>0.84382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364940</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>0.966870</td>\n",
       "      <td>-1.56740</td>\n",
       "      <td>-0.404540</td>\n",
       "      <td>-0.795570</td>\n",
       "      <td>-0.005053</td>\n",
       "      <td>0.021972</td>\n",
       "      <td>-0.73638</td>\n",
       "      <td>0.65277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.32909</td>\n",
       "      <td>0.402210</td>\n",
       "      <td>-0.810000</td>\n",
       "      <td>-0.373790</td>\n",
       "      <td>-0.910750</td>\n",
       "      <td>-1.49610</td>\n",
       "      <td>1.71340</td>\n",
       "      <td>0.902960</td>\n",
       "      <td>-0.930520</td>\n",
       "      <td>0.23089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>-0.191760</td>\n",
       "      <td>-1.686800</td>\n",
       "      <td>-0.40934</td>\n",
       "      <td>-0.533340</td>\n",
       "      <td>-0.277250</td>\n",
       "      <td>-0.865530</td>\n",
       "      <td>-0.163710</td>\n",
       "      <td>0.89115</td>\n",
       "      <td>-0.74183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13355</th>\n",
       "      <td>-1.11140</td>\n",
       "      <td>-0.280430</td>\n",
       "      <td>1.616600</td>\n",
       "      <td>0.138530</td>\n",
       "      <td>-0.723320</td>\n",
       "      <td>-1.07560</td>\n",
       "      <td>-0.23833</td>\n",
       "      <td>-1.146600</td>\n",
       "      <td>0.457850</td>\n",
       "      <td>-1.92520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815150</td>\n",
       "      <td>0.182940</td>\n",
       "      <td>0.144070</td>\n",
       "      <td>0.25641</td>\n",
       "      <td>1.130300</td>\n",
       "      <td>0.791030</td>\n",
       "      <td>1.032600</td>\n",
       "      <td>-0.808720</td>\n",
       "      <td>-0.37643</td>\n",
       "      <td>0.45173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13356</th>\n",
       "      <td>-0.19519</td>\n",
       "      <td>-1.341900</td>\n",
       "      <td>0.997720</td>\n",
       "      <td>-0.807040</td>\n",
       "      <td>0.079999</td>\n",
       "      <td>-1.22180</td>\n",
       "      <td>0.93940</td>\n",
       "      <td>1.719000</td>\n",
       "      <td>0.819370</td>\n",
       "      <td>1.20430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110580</td>\n",
       "      <td>1.593800</td>\n",
       "      <td>-0.143640</td>\n",
       "      <td>-0.47006</td>\n",
       "      <td>-0.056109</td>\n",
       "      <td>-1.057300</td>\n",
       "      <td>1.351200</td>\n",
       "      <td>0.286440</td>\n",
       "      <td>1.27390</td>\n",
       "      <td>-0.90654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13357</th>\n",
       "      <td>-0.79130</td>\n",
       "      <td>-0.315040</td>\n",
       "      <td>0.674080</td>\n",
       "      <td>0.145770</td>\n",
       "      <td>-0.785070</td>\n",
       "      <td>0.12570</td>\n",
       "      <td>-0.22798</td>\n",
       "      <td>0.320850</td>\n",
       "      <td>0.161670</td>\n",
       "      <td>-1.32910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091453</td>\n",
       "      <td>-0.248020</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>0.73830</td>\n",
       "      <td>1.114000</td>\n",
       "      <td>-0.525470</td>\n",
       "      <td>0.329720</td>\n",
       "      <td>0.042594</td>\n",
       "      <td>-0.26669</td>\n",
       "      <td>-0.38621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13358</th>\n",
       "      <td>-0.40480</td>\n",
       "      <td>0.223590</td>\n",
       "      <td>1.316500</td>\n",
       "      <td>-0.957940</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>-0.48804</td>\n",
       "      <td>1.85070</td>\n",
       "      <td>0.198420</td>\n",
       "      <td>-0.630100</td>\n",
       "      <td>-0.61487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423620</td>\n",
       "      <td>0.559940</td>\n",
       "      <td>-0.864110</td>\n",
       "      <td>-0.46372</td>\n",
       "      <td>1.258700</td>\n",
       "      <td>-0.457300</td>\n",
       "      <td>0.115480</td>\n",
       "      <td>0.532610</td>\n",
       "      <td>-0.50871</td>\n",
       "      <td>0.34216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13359</th>\n",
       "      <td>-1.09320</td>\n",
       "      <td>-0.003725</td>\n",
       "      <td>0.570640</td>\n",
       "      <td>0.649390</td>\n",
       "      <td>-0.590840</td>\n",
       "      <td>0.50037</td>\n",
       "      <td>0.26718</td>\n",
       "      <td>-0.723050</td>\n",
       "      <td>0.858430</td>\n",
       "      <td>0.38984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.566320</td>\n",
       "      <td>1.535200</td>\n",
       "      <td>-0.551870</td>\n",
       "      <td>0.49801</td>\n",
       "      <td>0.379850</td>\n",
       "      <td>-0.083597</td>\n",
       "      <td>0.797200</td>\n",
       "      <td>-0.687790</td>\n",
       "      <td>-0.17282</td>\n",
       "      <td>0.96402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13360 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1         2         3         4         5        6        7   \\\n",
       "0      0.84705 -1.034900 -0.050419  0.271640 -0.586590  0.99514  0.25267   \n",
       "1      0.69586 -1.146900 -0.417970 -0.022311 -0.023801  0.82358  1.22280   \n",
       "2     -0.98126  0.458230  0.201170 -0.405540 -0.976080 -0.54941  0.94278   \n",
       "3     -0.32929 -0.160370  0.107850 -0.396100 -0.488270 -0.17528  0.23056   \n",
       "4     -0.32909  0.402210 -0.810000 -0.373790 -0.910750 -1.49610  1.71340   \n",
       "...        ...       ...       ...       ...       ...      ...      ...   \n",
       "13355 -1.11140 -0.280430  1.616600  0.138530 -0.723320 -1.07560 -0.23833   \n",
       "13356 -0.19519 -1.341900  0.997720 -0.807040  0.079999 -1.22180  0.93940   \n",
       "13357 -0.79130 -0.315040  0.674080  0.145770 -0.785070  0.12570 -0.22798   \n",
       "13358 -0.40480  0.223590  1.316500 -0.957940  0.280500 -0.48804  1.85070   \n",
       "13359 -1.09320 -0.003725  0.570640  0.649390 -0.590840  0.50037  0.26718   \n",
       "\n",
       "             8         9        10  ...        16        17        18  \\\n",
       "0      1.696300  0.103130  0.80073  ... -0.670150 -0.648120  0.010373   \n",
       "1      1.741000 -0.909790  1.37250  ... -0.570580 -0.508610 -0.165750   \n",
       "2      0.072537 -0.371080  0.12274  ...  0.723650 -0.501790 -0.495010   \n",
       "3     -0.491150 -0.065798  0.84382  ...  0.364940 -0.004254  0.966870   \n",
       "4      0.902960 -0.930520  0.23089  ...  0.194300 -0.191760 -1.686800   \n",
       "...         ...       ...      ...  ...       ...       ...       ...   \n",
       "13355 -1.146600  0.457850 -1.92520  ...  0.815150  0.182940  0.144070   \n",
       "13356  1.719000  0.819370  1.20430  ...  0.110580  1.593800 -0.143640   \n",
       "13357  0.320850  0.161670 -1.32910  ... -0.091453 -0.248020 -0.061035   \n",
       "13358  0.198420 -0.630100 -0.61487  ...  0.423620  0.559940 -0.864110   \n",
       "13359 -0.723050  0.858430  0.38984  ... -0.566320  1.535200 -0.551870   \n",
       "\n",
       "            19        20        21        22        23       24       25  \n",
       "0     -0.71889 -0.749970  0.248620  0.103190 -1.173200  0.58196  0.33846  \n",
       "1     -0.98153 -0.821300  0.243330 -0.144820 -0.678770  0.70610  0.40833  \n",
       "2     -1.10370 -0.997580  0.277280  0.218630  1.034900  0.16152  0.37009  \n",
       "3     -1.56740 -0.404540 -0.795570 -0.005053  0.021972 -0.73638  0.65277  \n",
       "4     -0.40934 -0.533340 -0.277250 -0.865530 -0.163710  0.89115 -0.74183  \n",
       "...        ...       ...       ...       ...       ...      ...      ...  \n",
       "13355  0.25641  1.130300  0.791030  1.032600 -0.808720 -0.37643  0.45173  \n",
       "13356 -0.47006 -0.056109 -1.057300  1.351200  0.286440  1.27390 -0.90654  \n",
       "13357  0.73830  1.114000 -0.525470  0.329720  0.042594 -0.26669 -0.38621  \n",
       "13358 -0.46372  1.258700 -0.457300  0.115480  0.532610 -0.50871  0.34216  \n",
       "13359  0.49801  0.379850 -0.083597  0.797200 -0.687790 -0.17282  0.96402  \n",
       "\n",
       "[13360 rows x 25 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>25075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>75589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>come</td>\n",
       "      <td>1322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>in</td>\n",
       "      <td>14057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>please</td>\n",
       "      <td>1272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13355</th>\n",
       "      <td>13355</td>\n",
       "      <td>montanna</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13356</th>\n",
       "      <td>13356</td>\n",
       "      <td>pala</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13357</th>\n",
       "      <td>13357</td>\n",
       "      <td>calvin</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13358</th>\n",
       "      <td>13358</td>\n",
       "      <td>throwback</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13359</th>\n",
       "      <td>13359</td>\n",
       "      <td>tuna</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13360 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_index       word  word_count\n",
       "0               0          ,       25075\n",
       "1               1          .       75589\n",
       "2               2       come        1322\n",
       "3               3         in       14057\n",
       "4               4     please        1272\n",
       "...           ...        ...         ...\n",
       "13355       13355   montanna           3\n",
       "13356       13356       pala           3\n",
       "13357       13357     calvin           3\n",
       "13358       13358  throwback           3\n",
       "13359       13359       tuna           3\n",
       "\n",
       "[13360 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the token matrix from the sentences\n",
    "\n",
    "token_matrix = np.zeros((len(token_seqs), MAXLEN))\n",
    "for i, token_seq in enumerate(token_seqs):\n",
    "    token_matrix[i] = token_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01446164,  0.4703781 ,  0.43932216,  0.55233676,  0.02891891,\n",
       "         0.92247504,  0.20379483,  0.74396067,  0.87295721,  0.32212421,\n",
       "         0.54908864,  0.79422168,  0.72315875,  0.77089404,  0.06803678,\n",
       "         0.9024972 ,  0.18827808,  0.9008275 ,  0.678065  ,  0.70644177,\n",
       "         0.69787192,  0.858033  ,  0.12529439,  0.24855894,  0.53692943],\n",
       "       [ 0.10950312,  0.28796521,  0.46431358,  0.89759107,  0.37106506,\n",
       "         0.81462702,  0.814373  ,  0.70862121,  0.12932456,  0.16117888,\n",
       "         0.54312971,  0.47464406,  0.51010857,  0.52814441,  0.73896713,\n",
       "         0.29778308,  0.38799261,  0.7065847 ,  0.51547727,  0.21530675,\n",
       "         0.7970639 ,  0.02145494,  0.20971331,  0.53879638,  0.44602427],\n",
       "       [ 0.84705   , -1.0349    , -0.050419  ,  0.27164   , -0.58659   ,\n",
       "         0.99514   ,  0.25267   ,  1.6963    ,  0.10313   ,  0.80073   ,\n",
       "         0.74655   , -1.2667    , -4.036     , -0.22557   ,  0.16322   ,\n",
       "        -0.67015   , -0.64812   ,  0.010373  , -0.71889   , -0.74997   ,\n",
       "         0.24862   ,  0.10319   , -1.1732    ,  0.58196   ,  0.33846   ],\n",
       "       [ 0.69586   , -1.1469    , -0.41797   , -0.022311  , -0.023801  ,\n",
       "         0.82358   ,  1.2228    ,  1.741     , -0.90979   ,  1.3725    ,\n",
       "         0.1153    , -0.63906   , -3.2252    ,  0.61269   ,  0.33544   ,\n",
       "        -0.57058   , -0.50861   , -0.16575   , -0.98153   , -0.8213    ,\n",
       "         0.24333   , -0.14482   , -0.67877   ,  0.7061    ,  0.40833   ],\n",
       "       [-0.98126   ,  0.45823   ,  0.20117   , -0.40554   , -0.97608   ,\n",
       "        -0.54941   ,  0.94278   ,  0.072537  , -0.37108   ,  0.12274   ,\n",
       "        -0.19509   , -1.8045    , -4.6733    ,  0.029757  , -0.32505   ,\n",
       "         0.72365   , -0.50179   , -0.49501   , -1.1037    , -0.99758   ,\n",
       "         0.27728   ,  0.21863   ,  1.0349    ,  0.16152   ,  0.37009   ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.random.random((25, 2)), \n",
    "  token_embeddings[0:3].T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Conv1D, Dropout, Concatenate, Flatten, MaxPooling1D, Dense, Input\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Neural Network which uses Bidirectional LSTMs and convolutional layers \n",
    "# to will hopefully extract some of the contextual information \n",
    "# from the sentence. A BiLSTM can get information from forward and backward\n",
    "# states and the convolutions should also help capture some of context.\n",
    "def create_model(learning_rate, filter_size, out_size):\n",
    "    # an embeded sentence\n",
    "    inputs = Input((MAXLEN,), dtype='int32') \n",
    "    \n",
    "    embedded = Embedding(len(top_words),\n",
    "                            EMBEDDING_DIM,\n",
    "                            # add an extra row for 0 in the embedding, since 0 in the token sequences\n",
    "                            # represents an unknown elemement, and 1 represents the first element\n",
    "                            weights=[token_embeddings],\n",
    "                            input_length=MAXLEN,\n",
    "                            trainable=True)(inputs)\n",
    "    \n",
    "    # LSTMs\n",
    "    l_d0 = Dense(1024, activation='relu')(embedded)\n",
    "    l_d_drop = Dropout(0.2)(l_d0)\n",
    "    l_lstm1 = Bidirectional(LSTM(12,return_sequences=True,dropout=0.15, recurrent_dropout=0.0))(l_d_drop)\n",
    "    l_lstm2 = Bidirectional(LSTM(12,return_sequences=True,dropout=0.15, recurrent_dropout=0.0))(l_lstm1)\n",
    "    l_lstm_last = Bidirectional(LSTM(12,return_sequences=True,dropout=0.15, recurrent_dropout=0.0))(l_lstm2)\n",
    "    \n",
    "    # Covolutions\n",
    "    l_conv_4f = Conv1D(filters=filter_size,kernel_size=4,activation='relu',kernel_regularizer=regularizers.l2(0.0001))(embedded)\n",
    "    l_conv_4f = Dropout(0.2)(l_conv_4f)\n",
    "\n",
    "    l_conv_3f = Conv1D(filters=filter_size,kernel_size=3,activation='relu',)(embedded)\n",
    "    l_conv_3f = Dropout(0.2)(l_conv_3f)\n",
    "\n",
    "    l_conv_2f = Conv1D(filters=filter_size,kernel_size=2,activation='relu')(embedded)\n",
    "    l_conv_2f = Dropout(0.2)(l_conv_2f)\n",
    "\n",
    "    conv_2 = [l_conv_4f, l_conv_3f, l_conv_2f]\n",
    "\n",
    "    l_merge_2 = Concatenate(axis=1)(conv_2)\n",
    "    l_c_lstm = Bidirectional(LSTM(12,return_sequences=True,dropout=0.3, recurrent_dropout=0.0))(l_merge_2)\n",
    "\n",
    "    # Merge LSTM and Covolutions\n",
    "    l_merge = Concatenate(axis=1)([l_lstm_last, l_c_lstm])\n",
    "    l_pool = MaxPooling1D(4)(l_c_lstm)\n",
    "    l_drop = Dropout(0.2)(l_pool)\n",
    "    \n",
    "    l_flat = Flatten()(l_drop) #l_drop)\n",
    "    l_dense = Dense(256, activation='relu')(l_flat)\n",
    "    l_drop = Dropout(0.2)(l_dense)\n",
    "    l_dense2 = Dense(128, activation='relu')(l_drop)\n",
    "    l_dense_last = Dense(32, activation='relu')(l_dense2)\n",
    "    \n",
    "    output = Dense(out_size,\n",
    "                          activation='softmax')(l_dense)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], learning_rate=learning_rate)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76950 samples, validate on 4050 samples\n",
      "Epoch 1/15\n",
      "76950/76950 [==============================] - 30s 392us/sample - loss: 1.2539 - accuracy: 0.5404 - val_loss: 1.0630 - val_accuracy: 0.6175\n",
      "Epoch 2/15\n",
      "76950/76950 [==============================] - 25s 330us/sample - loss: 1.0198 - accuracy: 0.6296 - val_loss: 0.9527 - val_accuracy: 0.6514\n",
      "Epoch 3/15\n",
      "76950/76950 [==============================] - 26s 335us/sample - loss: 0.9146 - accuracy: 0.6694 - val_loss: 0.8306 - val_accuracy: 0.7104\n",
      "Epoch 4/15\n",
      "76950/76950 [==============================] - 26s 335us/sample - loss: 0.8208 - accuracy: 0.7095 - val_loss: 0.7925 - val_accuracy: 0.7277\n",
      "Epoch 5/15\n",
      "76950/76950 [==============================] - 26s 334us/sample - loss: 0.7664 - accuracy: 0.7278 - val_loss: 0.7725 - val_accuracy: 0.7294\n",
      "Epoch 6/15\n",
      "76950/76950 [==============================] - 25s 325us/sample - loss: 0.7318 - accuracy: 0.7402 - val_loss: 0.7588 - val_accuracy: 0.7343\n",
      "Epoch 7/15\n",
      "76950/76950 [==============================] - 25s 329us/sample - loss: 0.7080 - accuracy: 0.7467 - val_loss: 0.7907 - val_accuracy: 0.7326\n",
      "Epoch 8/15\n",
      "76950/76950 [==============================] - 26s 334us/sample - loss: 0.6916 - accuracy: 0.7514 - val_loss: 0.7426 - val_accuracy: 0.7365\n",
      "Epoch 9/15\n",
      "76950/76950 [==============================] - 26s 332us/sample - loss: 0.6700 - accuracy: 0.7583 - val_loss: 0.7462 - val_accuracy: 0.7373\n",
      "Epoch 10/15\n",
      "76950/76950 [==============================] - 25s 329us/sample - loss: 0.6545 - accuracy: 0.7637 - val_loss: 0.7706 - val_accuracy: 0.7304\n",
      "Epoch 11/15\n",
      "76950/76950 [==============================] - 25s 324us/sample - loss: 0.6414 - accuracy: 0.7674 - val_loss: 0.7659 - val_accuracy: 0.7346\n",
      "Epoch 12/15\n",
      "76950/76950 [==============================] - 25s 324us/sample - loss: 0.6281 - accuracy: 0.7720 - val_loss: 0.7652 - val_accuracy: 0.7336\n",
      "Epoch 13/15\n",
      "76950/76950 [==============================] - 25s 323us/sample - loss: 0.6129 - accuracy: 0.7771 - val_loss: 0.7591 - val_accuracy: 0.7365\n",
      "Epoch 14/15\n",
      "76950/76950 [==============================] - 25s 323us/sample - loss: 0.6025 - accuracy: 0.7824 - val_loss: 0.7834 - val_accuracy: 0.7316\n",
      "Epoch 15/15\n",
      "76950/76950 [==============================] - 25s 323us/sample - loss: 0.5914 - accuracy: 0.7864 - val_loss: 0.7939 - val_accuracy: 0.7289\n",
      "WARNING:tensorflow:From /home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: emote_model/assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "random_indexes = pd.DataFrame(token_matrix).sample(frac=0.9).index.to_numpy()\n",
    "\n",
    "# Set up the data for the model\n",
    "X = token_matrix[random_indexes]\n",
    "X_val = token_matrix[~random_indexes]\n",
    "le = LabelEncoder()\n",
    "y = utils.to_categorical(le.fit_transform(dd_train['emote']))\n",
    "y_train = y[random_indexes] \n",
    "y_val = y[~random_indexes]\n",
    "token_matrix[random_indexes]\n",
    "\n",
    "# Fit the model\n",
    "if needs_update:\n",
    "    model = create_model(learning_rate=0.001, filter_size=12, out_size=dd_train['emote'].nunique())\n",
    "    model.fit(X,y_train, verbose=True, epochs=15, batch_size=128, validation_split=0.05)\n",
    "    model.save('emote_model')\n",
    "else:\n",
    "    model = load_model('emote_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "y_preds = model.predict(X_val)\n",
    "y_pred_labels = np.array(list(map(lambda y_pred: np.argmax(y_pred), y_preds)))\n",
    "y_test_labels = dd_train.iloc[~random_indexes].emote.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_test_labels.shape == y_pred_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    true  pred\n",
       "0      0     0\n",
       "1      3     0\n",
       "2      5     5\n",
       "3      1     1\n",
       "4      4     0\n",
       "5      0     0\n",
       "6      5     5\n",
       "7      3     3\n",
       "8      5     5\n",
       "9      0     0\n",
       "10     0     0\n",
       "11     0     0\n",
       "12     3     3\n",
       "13     1     1\n",
       "14     6     6\n",
       "15     0     0\n",
       "16     4     4\n",
       "17     4     4\n",
       "18     0     0\n",
       "19     4     4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.DataFrame({'true':y_test_labels,'pred': y_pred_labels}, columns=['true', 'pred'])\n",
    "y_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.83     28779\n",
      "           1       0.92      0.89      0.90      4131\n",
      "           2       0.42      0.39      0.40      1003\n",
      "           3       0.65      0.61      0.63      8920\n",
      "           4       0.84      0.82      0.83     23740\n",
      "           5       0.88      0.74      0.80     11553\n",
      "           6       0.75      0.38      0.50      2874\n",
      "\n",
      "    accuracy                           0.80     81000\n",
      "   macro avg       0.75      0.67      0.70     81000\n",
      "weighted avg       0.80      0.80      0.79     81000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_df['true'], y_df['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6a097b0eb8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "cm = confusion_matrix(y_df['true'], y_df['pred'])\n",
    "sns.heatmap(cm / cm.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test = doc2token_seq('I hate everything aaa I hate you')\n",
    "pred = model.predict(np.array([my_test]))\n",
    "# todo tabulate\n",
    "# 0 no emotion\n",
    "# 1 anger\n",
    "# 2 disgust\n",
    "# 3 fear\n",
    "# 4 happiness\n",
    "# 5 sadness\n",
    "# 6 surprise\n",
    "\n",
    "# predict disgust\n",
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict no emotion\n",
    "my_test = doc2token_seq('Wish me luck!')\n",
    "pred = model.predict(np.array([my_test]))\n",
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicts sadness\n",
    "my_test = doc2token_seq('I am happy, but am I am sad.')\n",
    "pred = model.predict(np.array([my_test]))\n",
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicts fear\n",
    "my_test = doc2token_seq('the world is so cruel...')\n",
    "pred = model.predict(np.array([my_test]))\n",
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicts disgust\n",
    "my_test = doc2token_seq(\"you are such an idiot!!!\")\n",
    "pred = model.predict(np.array([my_test]))\n",
    "np.argmax(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
